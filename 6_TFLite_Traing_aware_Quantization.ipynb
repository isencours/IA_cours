{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "6_TFLite_Traing_aware_Quantization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSNnX5mbU0kE"
      },
      "source": [
        "## Download Tensorflow and Model Optimization packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0pZJRr977Vs",
        "outputId": "86c837f7-9640-4075-d6d0-479e64da379b"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install -q tf-nightly-gpu\n",
        "!pip install -q tensorflow-model-optimization==0.5\n",
        "!pip install tensorflow==2.4.0\n",
        "\n",
        "import tempfile\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.0:\n",
            "  Successfully uninstalled tensorflow-2.4.0\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "Collecting tensorflow==2.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/94/0a/012cc33c643d844433d13001dd1db179e7020b05ddbbd0a9dc86c38a8efa/tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.12.4)\n",
            "Collecting gast==0.3.3\n",
            "  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Collecting grpcio~=1.32.0\n",
            "  Using cached https://files.pythonhosted.org/packages/06/54/1c8be62beafe7fb1548d2968e518ca040556b46b0275399d4f3186c56d79/grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12.1)\n",
            "Collecting h5py~=2.10.0\n",
            "  Using cached https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow==2.4.0) (54.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.27.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "\u001b[31mERROR: tf-nightly-gpu 2.5.0.dev20210318 has requirement gast==0.4.0, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly-gpu 2.5.0.dev20210318 has requirement grpcio~=1.34.0, but you'll have grpcio 1.32.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly-gpu 2.5.0.dev20210318 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, grpcio, h5py, tensorflow\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: grpcio 1.34.1\n",
            "    Uninstalling grpcio-1.34.1:\n",
            "      Successfully uninstalled grpcio-1.34.1\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "Successfully installed gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 tensorflow-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FMJAMqnU9E3"
      },
      "source": [
        "## Download and split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yaE4wS__3Ao",
        "outputId": "f3236255-0e0b-4bb0-d290-709f372189fa"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE)) / 255.0\n",
        "    return  image, label\n",
        "\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    data_dir='/content/dataset/',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    shuffle_files=True, \n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "# tf.data.experimental.cardinality(raw_train_dataset).numpy()\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "print(\"Numbers of images: \", num_examples)\n",
        "print(\"Numbers of classes: \", num_classes)\n",
        "\n",
        "train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "test_batches = raw_test.map(format_image).batch(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numbers of images:  23262\n",
            "Numbers of classes:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M06FDJviVEu8"
      },
      "source": [
        "## Our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "318siK7H_6sG",
        "outputId": "c43ff5ff-5e38-405d-965e-e912fc476762"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    # Output neuron\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "print(model.summary())\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 222, 222, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 111, 111, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 109, 109, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 52, 52, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2359552   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 2,453,218\n",
            "Trainable params: 2,453,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4T_6crVMjO"
      },
      "source": [
        "## Training our Keras model without quantization aware training\n",
        "\n",
        "we train our Keras model with only one epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIrsBu8o_9qU",
        "outputId": "1437b064-a5ae-462e-8fae-e34525ab7c93"
      },
      "source": [
        "history = model.fit(\n",
        "      train_batches,  \n",
        "      epochs=1,\n",
        "      validation_data=validation_batches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "582/582 [==============================] - 63s 49ms/step - loss: 0.6822 - acc: 0.5866 - val_loss: 0.7314 - val_acc: 0.6367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fw5AO7bXEB-"
      },
      "source": [
        "##Save the Tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbRze7xTW-sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc59882-c627-47a5-ac1c-4a25e68a6873"
      },
      "source": [
        "CATS_VS_DOGS_SAVED_MODEL = \"./exp_saved_model\"\n",
        "tf.saved_model.save(model, CATS_VS_DOGS_SAVED_MODEL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./exp_saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./exp_saved_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNlNNG7RW7KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00e2dcb-b6d5-41b0-b822-e930f6dbc8af"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "def du(path):\n",
        "    return subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8')\n",
        "# Get file size in bytes for a given model\n",
        "print(\"Model size in Mb:\")\n",
        "print(du(CATS_VS_DOGS_SAVED_MODEL))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model size in Mb:\n",
            "19M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coQ4cbxMVoZ2"
      },
      "source": [
        "#Clone and fine-tune pre-trained model with quantization aware training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMbEQ3STACn-",
        "outputId": "c254adde-bf2d-4f04-ef49-13adb76069d0"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()\n",
        "q_aware_history = q_aware_model.fit(\n",
        "      train_batches,  \n",
        "      epochs=1,\n",
        "      validation_data=validation_batches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quantize_layer (QuantizeLaye (None, 224, 224, 3)       3         \n",
            "_________________________________________________________________\n",
            "quant_conv2d (QuantizeWrappe (None, 222, 222, 16)      483       \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 111, 111, 16)      1         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_1 (QuantizeWrap (None, 109, 109, 32)      4707      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 54, 54, 32)        1         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_2 (QuantizeWrap (None, 52, 52, 64)        18627     \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 26, 26, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_3 (QuantizeWrap (None, 24, 24, 64)        37059     \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_3 (Quant (None, 12, 12, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_flatten (QuantizeWrapp (None, 9216)              1         \n",
            "_________________________________________________________________\n",
            "quant_dense (QuantizeWrapper (None, 256)               2359557   \n",
            "_________________________________________________________________\n",
            "quant_dense_1 (QuantizeWrapp (None, 128)               32901     \n",
            "_________________________________________________________________\n",
            "quant_dense_2 (QuantizeWrapp (None, 2)                 263       \n",
            "=================================================================\n",
            "Total params: 2,453,605\n",
            "Trainable params: 2,453,218\n",
            "Non-trainable params: 387\n",
            "_________________________________________________________________\n",
            "582/582 [==============================] - 37s 55ms/step - loss: 0.5170 - accuracy: 0.7435 - val_loss: 0.4462 - val_accuracy: 0.8040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIJqkJsIVsfO"
      },
      "source": [
        "## Evaluate the model against baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzGXkg9bAVCw",
        "outputId": "f930a4f0-1582-4a76-93df-3812c8e6187e"
      },
      "source": [
        "_, baseline_model_accuracy = model.evaluate(test_batches, verbose=0)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(test_batches, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline test accuracy: 0.649613082408905\n",
            "Quant test accuracy: 0.8048151135444641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCDNAAsgV-vv"
      },
      "source": [
        "## Create quantized model for TFLite backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQV6xMi9XzfY"
      },
      "source": [
        "Quantized model for the baseline Keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta_wyvLIX_2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ec052a-8688-4dab-d0fd-7dcd7fdef185"
      },
      "source": [
        "# Create float TFLite model.\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "# Measure sizes of models.\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpfad0amis/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpfad0amis/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Float model in Mb: 9.362892150878906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K0ag3XlYMZv"
      },
      "source": [
        "Quantized model for the QAT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGe03_gsAnsL",
        "outputId": "daa2a948-5962-4a1b-e0c3-d6b20d178b68"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpgix8upxz/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpgix8upxz/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Quantized model in Mb: 2.351898193359375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkzeKS0jZFA6"
      },
      "source": [
        "## Note\n",
        "\n",
        "What can we see in term of the size and the accuracy of models ? We can try some modification to assess the performance:\n",
        "\n",
        "    By default: converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    Opimization for size: Hint : [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "    Opimization for latency: Hint : [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "651IgjLyo-Jx"
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}